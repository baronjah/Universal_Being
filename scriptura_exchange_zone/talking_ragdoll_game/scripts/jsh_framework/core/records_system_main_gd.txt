#
# JSH Ethereal Records System

#

#      oooo  .oooooo..o ooooo   ooooo 
#      `888 d8P'    `Y8 `888'   `888' 
#       888 Y88bo.       888     888     ┏┓ ┓         ┓  ┳┓        ┓   ┏┓         
#       888  `"Y8888o.   888ooooo888     ┣ ╋┣┓┏┓┏┓┏┓┏┓┃  ┣┫┏┓┏┏┓┏┓┏┫┏  ┗┓┓┏┏╋┏┓┏┳┓
#       888      `"Y88b  888     888     ┗┛┗┛┗┗ ┛ ┗ ┗┻┗  ┛┗┗ ┗┗┛┛ ┗┻┛  ┗┛┗┫┛┗┗ ┛┗┗
#       888 oo     .d8P  888     888                                      ┛       
#   .o. 88P 8""88888P'  o888o   o888o 
#   `Y888P                            

#

####################
# Check Records

#               ,,                                                                                         ,,          
#   .g8"""bgd `7MM                       `7MM          `7MM"""Mq.                                        `7MM          
# .dP'     `M   MM                         MM            MM   `MM.                                         MM          
# dM'       `   MMpMMMb.  .gP"Ya   ,p6"bo  MM  ,MP'      MM   ,M9  .gP"Ya   ,p6"bo   ,pW"Wq.`7Mb,od8  ,M""bMM  ,pP"Ybd 
# MM            MM    MM ,M'   Yb 6M'  OO  MM ;Y         MMmmdM9  ,M'   Yb 6M'  OO  6W'   `Wb MM' "',AP    MM  8I   `" 
# MM.           MM    MM 8M"""""" 8M       MM;Mm         MM  YM.  8M"""""" 8M       8M     M8 MM    8MI    MM  `YMMMa. 
# `Mb.     ,'   MM    MM YM.    , YM.    , MM `Mb.       MM   `Mb.YM.    , YM.    , YA.   ,A9 MM    `Mb    MM  L.   I8 
#   `"bmmmd'  .JMML  JMML.`Mbmmd'  YMbmd'.JMML. YA.    .JMML. .JMM.`Mbmmd'  YMbmd'   `Ybmd9'.JMML.   `Wbmd"MML.M9mmmP' 

## check if active has that set
## check if active set is empty
# -1 = missing, 0 = empty, 1 = it is here

func check_record_in_active(records_set_name):
	active_r_s_mut.lock()
	# -1 = dont have that key, it is empty
	var active_int : int = -1
	if active_record_sets.has(records_set_name):
		# 0 = has that key
		active_int = 0
		if !active_record_sets[records_set_name].is_empty():
			# 1 = has that key, not empty
			active_int = 1
	active_r_s_mut.unlock()
	return active_int

func check_record_in_cached(records_set_name):
	cached_r_s_mutex.lock()
	# -1 = dont have that key, it is empty
	var cached_int : int = -1
	if cached_record_sets.has(records_set_name):
		# 0 = has that key
		cached_int = 0
		if !cached_record_sets[records_set_name].is_empty():
			# 1 = has that key, not empty
			cached_int = 1
	cached_r_s_mutex.unlock()
	return cached_int

func check_set_limit(records_set_name):
	var currently_checked_set : int = -1
	var set_limit = BanksCombiner.dataSetLimits
	if set_limit.has(records_set_name):
		currently_checked_set = set_limit[records_set_name]
	return currently_checked_set

func check_current_set_container_count(record_set_name):
	active_r_s_mut.lock()
	var current_container_count : int = -1
	if active_record_sets.has(record_set_name):
		if active_record_sets[record_set_name].has("metadata"):
			if active_record_sets[record_set_name]["metadata"].has("container_count"):
				current_container_count = active_record_sets[record_set_name]["metadata"]["container_count"]
	active_r_s_mut.unlock()
	return current_container_count

func check_record_set_type(record_set_name):
	var currently_checked_set : int = -1
	var set_type = BanksCombiner.data_set_type
	if set_type.has(record_set_name):
		currently_checked_set = set_type[record_set_name]
	var set_type_string : String = ""
	if currently_checked_set == -1:
		print( " we didnt find it, lets put a 0 here ")
		currently_checked_set = 0
		set_type_string = "single"
	elif currently_checked_set == 0:
		print(" single ")
		set_type_string = "single"
	elif currently_checked_set == 1:
		print(" multi ")
		set_type_string = "multi"
	elif currently_checked_set == 2:
		print(" duplicate")
		set_type_string = "duplicate"
	return set_type_string

func check_if_first_time(set_name_first, the_current_of_energy):
	mutex_containers.lock()
	print(" set_name_first  : " , set_name_first , " the_current_of_energy ", the_current_of_energy , " list_of_containers " , list_of_containers)
	if list_of_containers.has(set_name_first):
		print(" it have it already")
	else:
		list_of_containers[set_name_first] = {}
		list_of_containers[set_name_first]["status"] = the_current_of_energy
	list_of_containers
	mutex_containers.unlock()

func containers_states_checker():
	mutex_for_container_state.lock()
	if current_containers_state.size() > 0:
		print("checkerrr bigger list than 0 ")
		for data_sets_to_check in current_containers_state:
			print(" alkaida is calling fbi xd :  ", data_sets_to_check , ", " , current_containers_state[data_sets_to_check]["status"])
			var state_of_check_0 : int = -1
			var state_of_check_1 : int = -1
			var state_of_check_2 : int = -1
			var vector_now : Vector3i
			vector_now.x = state_of_check_0
			vector_now.y = state_of_check_1
			vector_now.z = state_of_check_2
			if !current_containers_state[data_sets_to_check].has("status_tree"):
				current_containers_state[data_sets_to_check]["status_tree"] = "pending"
				current_containers_state[data_sets_to_check]["three_i"] = vector_now
			if current_containers_state[data_sets_to_check]["status"] == -1:
				print(" we must reset the xyz thingy")
				current_containers_state[data_sets_to_check]["three_i"] = vector_now
			var set_name_plus = data_sets_to_check + "_"
			var container_name_from_data_set : String = ""
			var datapoint_node_now : Node
			var container_node_now : Node
			var data_array_now : Array = []
			var dictionary_size_now : int
			active_r_s_mut.lock()
			if active_record_sets.has(set_name_plus):
				var plus_records = set_name_plus + "records"
				current_containers_state[data_sets_to_check]["status_tree"] = "started_0"
				if active_record_sets[set_name_plus].has(plus_records):
					current_containers_state[data_sets_to_check]["status_tree"] = "started_1"
					if active_record_sets[set_name_plus][plus_records].has("content"):
						if active_record_sets[set_name_plus][plus_records]["content"] is Array:
							print(" hmm " , active_record_sets[set_name_plus][plus_records]["content"])
							if active_record_sets[set_name_plus][plus_records]["content"] != []:
								if active_record_sets[set_name_plus][plus_records]["content"][0][0][3][0] is String:
									if active_record_sets[set_name_plus][plus_records]["content"][0][0][3][0] != "container":
										container_name_from_data_set = active_record_sets[set_name_plus][plus_records]["content"][0][0][5][0]
										current_containers_state[data_sets_to_check]["status_tree"] = "started_2"
									else:
										container_name_from_data_set = active_record_sets[set_name_plus][plus_records]["content"][0][0][6][0]
										current_containers_state[data_sets_to_check]["status_tree"] = "started_3"
								else:
									print(" FATAL KURWA ERROR, 1")
							else:
								print(" FATAL KURWA ERROR, 2",  active_record_sets[set_name_plus][plus_records]["content"])
						else:
							print(" FATAL KURWA ERROR, 3")
					else:
						print(" FATAL KURWA ERROR, VERY IMPORTANT, DUNNO WHY IT HAPPENED, OH MY ")
				else:
					print(" FATAL KURWA ERROR, 0")
				state_of_check_0 = 1
				current_containers_state[data_sets_to_check]["three_i"].x = state_of_check_0
				active_r_s_mut.unlock()
			else:
				active_r_s_mut.unlock()
				cached_r_s_mutex.lock()
				if cached_record_sets.has(set_name_plus):
					var plus_records = set_name_plus + "records"
					current_containers_state[data_sets_to_check]["status_tree"] = "cached_0"
					if cached_record_sets[set_name_plus].has(plus_records):
						current_containers_state[data_sets_to_check]["status_tree"] = "cached_1"
						if cached_record_sets[set_name_plus][plus_records]["content"][0][0][3][0] != "container":
							container_name_from_data_set = cached_record_sets[set_name_plus][plus_records]["content"][0][0][5][0]
							current_containers_state[data_sets_to_check]["status_tree"] = "cached_2"
						else:
							container_name_from_data_set = cached_record_sets[set_name_plus][plus_records]["content"][0][0][6][0]
							current_containers_state[data_sets_to_check]["status_tree"] = "cached_3"
					state_of_check_0 = 0
					current_containers_state[data_sets_to_check]["three_i"].x = state_of_check_0
					cached_r_s_mutex.unlock()
				else:
					cached_r_s_mutex.unlock()
			if state_of_check_0 != -1:
				var container_name
				if container_name_from_data_set != "":
					container_name = container_name_from_data_set
				else:
					container_name = data_sets_to_check + "_container"
				current_containers_state[data_sets_to_check]["container_name"] = container_name
				tree_mutex.lock()
				if scene_tree_jsh.has("main_root"):
					if scene_tree_jsh["main_root"]["branches"].has(container_name):
						current_containers_state[data_sets_to_check]["status_tree"] = "started_4"
						if scene_tree_jsh["main_root"]["branches"][container_name].has("node"):
							print(" it has node, do we unload there ? nah it can go both ways")
							if is_instance_valid(scene_tree_jsh["main_root"]["branches"][container_name]["node"]):
								container_node_now = scene_tree_jsh["main_root"]["branches"][container_name]["node"]
							else:
								container_node_now = null
						else:
							container_node_now = null
							state_of_check_2 = -1
						if container_node_now:
							state_of_check_2 = 0
							current_containers_state[data_sets_to_check]["three_i"].z = state_of_check_2
							var container_name_for_trick = scene_tree_jsh["main_root"]["branches"][container_name]["name"]
							current_containers_state[data_sets_to_check]["status_tree"] = "started_5"
							var datapoint_path_now = scene_tree_jsh["main_root"]["branches"][container_name]["datapoint"]["datapoint_path"]
							datapoint_node_now = get_node(datapoint_path_now)
							if datapoint_node_now:
								current_containers_state[data_sets_to_check]["status_tree"] = "started_6"
								state_of_check_2 = 1
								current_containers_state[data_sets_to_check]["three_i"].z = state_of_check_2
								if datapoint_node_now.has_method("check_state_of_dictionary_and_three_ints_of_doom"):
									var data_array_now_ = datapoint_node_now.check_state_of_dictionary_and_three_ints_of_doom()
									current_containers_state[data_sets_to_check]["status_tree"] = "started_7"
									if data_array_now_ != null:
										data_array_now = data_array_now_
										if data_array_now[0] is Dictionary:
											current_containers_state[data_sets_to_check]["status_tree"] = "started_8"
											state_of_check_2 = 2
											current_containers_state[data_sets_to_check]["three_i"].z = state_of_check_2
						state_of_check_1 = 1
						current_containers_state[data_sets_to_check]["three_i"].y = state_of_check_1
						tree_mutex.unlock()
					else:
						tree_mutex.unlock()
						cached_tree_mutex.lock()
						if cached_jsh_tree_branches.has(container_name):
							current_containers_state[data_sets_to_check]["status_tree"] = "cached_4"
							state_of_check_1 = 0
							current_containers_state[data_sets_to_check]["three_i"].y = state_of_check_1
							cached_tree_mutex.unlock()
						else:
							mutex_for_trickery.lock()
							menace_tricker_checker = 1
							mutex_for_trickery.unlock()
							cached_tree_mutex.unlock()
				else:
					current_containers_state[data_sets_to_check]["status_tree"] = "fatal_kurwa_error"
					tree_mutex.unlock()
			if state_of_check_1 != -1:
				if state_of_check_2 != -1:
					print(" we even got nodes to tinker with")
			print(" alkaida is calling fbi xd :  ", data_sets_to_check , ", " , current_containers_state[data_sets_to_check]["three_i"])
	mutex_for_container_state.unlock()
	mutex_for_trickery.lock()
	if menace_tricker_checker == 2:
		print(" check is finished and we didnt get interupted while doing so kurwa ")
		menace_tricker_checker = 3
	mutex_for_trickery.unlock()

func the_basic_sets_creation():
	check_if_every_basic_set_is_loaded()
	if test_of_set_list_flow.size() > 0:
		print(" it is bigger than 0, we have sets to create ")

func get_every_basic_set():
	return BanksCombiner.data_sets_names_0

func get_every_basic_set_():
	return BanksCombiner.data_sets_names

func check_if_every_basic_set_is_loaded():
	var set_to_pull_now = JSH_records_system.check_basic_set_if_loaded()
	if set_to_pull_now:
		test_of_set_list_flow.append(set_to_pull_now)
		print(" set to pull now is : " , set_to_pull_now)

func container_finder(set_name):
	var wordly_word = set_name + BanksCombiner.data_names_0[0]
	active_r_s_mut.lock()
	var container_name_now = "akashic_records"
	if active_record_sets.has(set_name):
		if active_record_sets[set_name][wordly_word].has("content") and active_record_sets[set_name][wordly_word]["content"] != []:
			active_record_sets[set_name][wordly_word]
			container_name_now = active_record_sets[set_name][wordly_word]["content"][0][0][6][0]
	active_r_s_mut.unlock()
	var container_splitter = container_name_now.split("/")
	if container_splitter.size() > 1:
		container_name_now = container_splitter[0]
	return container_name_now

func initialize_menu(record_type: String):
	var records_set_name = record_type + "_"
	var use_active : bool = false
	var use_cached : bool = false
	var can_be_created : bool = false
	var state_of_active_check = check_record_in_active(records_set_name)
	var state_of_cached_check = check_record_in_cached(records_set_name)
	var set_limit_check = check_set_limit(records_set_name)
	var current_container_count_check = check_current_set_container_count(records_set_name)
	var record_set_type_check = check_record_set_type(records_set_name)
	print("catch current_container_count_check " , current_container_count_check, " in set " , record_type)
	print("catch initalize memories ! 0 : " , record_type)
	if state_of_active_check == 1:
		print("catch active is there ")
		use_active = true
	if state_of_cached_check == 1:
		print("catch cached have it ")
		use_cached = true
	if set_limit_check > current_container_count_check:
		print("catch we can still create ")
		can_be_created = true
	if can_be_created == true and use_active == false and use_cached == false:
		print("catch new checkers in initia menu new set must be created")
		create_record_from_script(record_type)
	elif can_be_created == true and use_active == true and use_cached == false:
		print("catch new checkers in initia menu it is in active, probably additional set can be created ", record_set_type_check)
		var additional_records_set_name = record_type + str(current_container_count_check)
		var aditional_cached = check_record_in_cached(additional_records_set_name)
		if aditional_cached == 1:
			print(" we had it in cache ")
			load_cached_record_to_active(additional_records_set_name)
			change_creation_set_name(record_type, additional_records_set_name)
			process_creation_further(additional_records_set_name, 1)
		else:
			print("catch some else ")
			create_additional_record_set(record_type, current_container_count_check)
			active_record_sets[records_set_name]["metadata"]["container_count"] +=1
			change_creation_set_name(record_type, additional_records_set_name)
			process_creation_further(additional_records_set_name, 1)
	elif can_be_created == true and use_active == false and use_cached == true:
		print("catch new checkers in initia menu it is in cached, we must move it around")
		load_cached_record_to_active(records_set_name)
		process_creation_further(record_type, 1)
	elif can_be_created == false:
		print("catch new checkers in initia menu we hit limit, cannot create more")
		process_creation_further(record_type, 7)


func find_record_set(record_type: String) -> Dictionary:
	match record_type:
		"base":
			return RecordsBank.records_map_0
		"menu":
			return RecordsBank.records_map_2
		"settings":
			return RecordsBank.records_map_3
		"keyboard":
			return RecordsBank.records_map_4
		"keyboard_left":
			return RecordsBank.records_map_5
		"keyboard_right":
			return RecordsBank.records_map_6
		"things_creation":
			return RecordsBank.records_map_7
		"singular_lines":
			return RecordsBank.records_map_8
		_:
			return {}

func find_instructions_set(record_type: String) -> Dictionary:
	match record_type:
		"base":
			return InstructionsBank.instructions_set_0
		"menu":
			return InstructionsBank.instructions_set_1
		"settings":
			return InstructionsBank.instructions_set_2
		"keyboard":
			return InstructionsBank.instructions_set_3
		"keyboard_left":
			return InstructionsBank.instructions_set_4
		"keyboard_right":
			return InstructionsBank.instructions_set_5
		"things_creation":
			return InstructionsBank.instructions_set_6
		"singular_lines":
			return InstructionsBank.instructions_set_7
		_:
			return {}

func find_scene_frames(record_type: String) -> Dictionary:
	match record_type:
		"base":
			return ScenesBank.scenes_frames_0
		"menu":
			return ScenesBank.scenes_frames_1
		"settings":
			return ScenesBank.scenes_frames_2
		"keyboard":
			return ScenesBank.scenes_frames_3
		"keyboard_left":
			return ScenesBank.scenes_frames_4
		"keyboard_right":
			return ScenesBank.scenes_frames_5
		"things_creation":
			return ScenesBank.scenes_frames_6
		"singular_lines":
			return ScenesBank.scenes_frames_7
		_:
			return {}

func find_interactions_list(record_type: String) -> Dictionary:
	match record_type:
		"base":
			return ActionsBank.interactions_list_0
		"menu":
			return ActionsBank.interactions_list_1
		"settings":
			return ActionsBank.interactions_list_2
		"keyboard":
			return ActionsBank.interactions_list_3
		"keyboard_left":
			return ActionsBank.interactions_list_4
		"keyboard_right":
			return ActionsBank.interactions_list_5
		"things_creation":
			return ActionsBank.interactions_list_6
		"singular_lines":
			return ActionsBank.interactions_list_7
		_:
			return {}

func record_mistake(mistake_data: Dictionary):
	history_tracking.mutex.lock()
	mistake_data["timestamp"] = Time.get_ticks_msec()
	history_tracking.mistakes.append(mistake_data)
	history_tracking.mutex.unlock()

func get_record_type_id(record_type: String) -> int:
	match record_type:
		"base":
			return 0
		"menu":
			return 1
		_:
			return -1

func get_cache_total_size() -> int:
	var total_size: int = 0
	cached_r_s_mutex.lock()
	for records_set in cached_record_sets:
		for record_type in cached_record_sets[records_set]:
			var data = cached_record_sets[records_set][record_type]
			total_size += get_dictionary_memory_size(data)
	cached_r_s_mutex.unlock()
	return total_size

func get_dictionary_memory_size(dict: Dictionary) -> int:
	var serialized = var_to_bytes(dict)
	return serialized.size()

func find_highest_in_array(numbers: Array) -> int:
	return numbers.max()

func new_function_for_creation_recovery(record_type_now, first_stage_of_creation_now, stage_of_creation_now):
	print(" fatal kurwa error 000 ", record_type_now , " , " , first_stage_of_creation_now, " , " , stage_of_creation_now)
	if load_queue_mutex.try_lock():
		print(" fatal kurwa error 00 load_queue_mutex ",)
	else:
		print(" fatal kurwa error 001 load_queue_mutex ",)
	array_with_no_mutex.append([record_type_now, first_stage_of_creation_now, stage_of_creation_now])

# Create Records

#                                                                                                           ,,          
#   .g8"""bgd                          mm               `7MM"""Mq.                                        `7MM          
# .dP'     `M                          MM                 MM   `MM.                                         MM          
# dM'       ``7Mb,od8 .gP"Ya   ,6"Yb.mmMMmm .gP"Ya        MM   ,M9  .gP"Ya   ,p6"bo   ,pW"Wq.`7Mb,od8  ,M""bMM  ,pP"Ybd 
# MM           MM' "',M'   Yb 8)   MM  MM  ,M'   Yb       MMmmdM9  ,M'   Yb 6M'  OO  6W'   `Wb MM' "',AP    MM  8I   `" 
# MM.          MM    8M""""""  ,pm9MM  MM  8M""""""       MM  YM.  8M"""""" 8M       8M     M8 MM    8MI    MM  `YMMMa. 
# `Mb.     ,'  MM    YM.    , 8M   MM  MM  YM.    ,       MM   `Mb.YM.    , YM.    , YA.   ,A9 MM    `Mb    MM  L.   I8 
#   `"bmmmd' .JMML.   `Mbmmd' `Moo9^Yo.`Mbmo`Mbmmd'     .JMML. .JMM.`Mbmmd'  YMbmd'   `Ybmd9'.JMML.   `Wbmd"MML.M9mmmP' 
#

func create_additional_record_set(record_type, current_container_count_check):
	var set_name_to_work_on = record_type + "_"
	active_r_s_mut.lock()
	var data_to_work_on_additional_set = active_record_sets[set_name_to_work_on].duplicate(true)
	active_r_s_mut.unlock()
	var datapoint_name_thing : String = ""
	var container_name_thing : String = ""
	var entire_array_of_scene_to_set : Array = []
	var amounts_of_instructions = data_to_work_on_additional_set[set_name_to_work_on + "instructions"]["content"].size() - 1
	amounts_of_instructions = amounts_of_instructions + current_container_count_check
	for stuffff in data_to_work_on_additional_set[set_name_to_work_on + "instructions"]["content"]:
		var type_of_instruction = stuffff[0][1][0]
		if type_of_instruction == "set_the_scene":
			entire_array_of_scene_to_set = stuffff
	var interaction_name = entire_array_of_scene_to_set[0][0][0]
	var new_interaction_split = interaction_name.split("_")
	var new_interaction_name = new_interaction_split[0] + "_" + str(amounts_of_instructions)
	entire_array_of_scene_to_set[0][0][0] = new_interaction_name
	entire_array_of_scene_to_set[1][2][0] = str(current_container_count_check)
	entire_array_of_scene_to_set[2][0][0] = str(current_container_count_check)
	data_to_work_on_additional_set[set_name_to_work_on + "instructions"] = {
		"header" = [new_interaction_name],
		"content" = [entire_array_of_scene_to_set]
	}
	data_to_work_on_additional_set[set_name_to_work_on + "scenes"]
	var scene_name_to_take = data_to_work_on_additional_set[set_name_to_work_on + "scenes"]["header"][current_container_count_check]
	var scene_array_to_take = data_to_work_on_additional_set[set_name_to_work_on + "scenes"]["content"][current_container_count_check]
	data_to_work_on_additional_set[set_name_to_work_on + "scenes"] = {
		"header" = [scene_name_to_take],
		"content" = [scene_array_to_take]
	}
	var amount_of_things = data_to_work_on_additional_set[set_name_to_work_on + "records"]["header"].size()
	var container_informations
	var first_counter : int = -1
	for stufff in data_to_work_on_additional_set[set_name_to_work_on + "records"]["content"]:
		first_counter +=1
		var thing_name_find = stufff[0][3][0]
		var thing_number_name_find = stufff[0][0][0]
		if thing_name_find == "container":
			container_name_thing = thing_number_name_find
			container_informations = data_to_work_on_additional_set[set_name_to_work_on + "records"]["content"][first_counter]
			data_to_work_on_additional_set[set_name_to_work_on + "records"]["content"][first_counter] = []
		elif thing_name_find == "datapoint":
			datapoint_name_thing = thing_number_name_find
			data_to_work_on_additional_set[set_name_to_work_on + "records"]["content"][first_counter] = []
		if container_name_thing != "" and datapoint_name_thing != "":
			break
	var container_name = container_informations[1][0][0]
	if datapoint_name_thing != "":
		data_to_work_on_additional_set[set_name_to_work_on + "records"]["content"].erase([])
		amount_of_things -=1
	if container_name_thing != "":
		data_to_work_on_additional_set[set_name_to_work_on + "records"]["content"].erase([])
		amount_of_things -=1
	amount_of_things = amount_of_things * current_container_count_check
	var second_counter : int = -1
	var counter_of_finish : int = 0
	for stufff_0 in data_to_work_on_additional_set[set_name_to_work_on + "records"]["header"]:
		second_counter +=1
		if stufff_0 == container_name_thing:
			data_to_work_on_additional_set[set_name_to_work_on + "records"]["header"][second_counter] = ""
			counter_of_finish +=1
		elif stufff_0 == datapoint_name_thing:
			data_to_work_on_additional_set[set_name_to_work_on + "records"]["header"][second_counter] = ""
			counter_of_finish +=1
		if counter_of_finish == 2:
			break
	if datapoint_name_thing != "":
		data_to_work_on_additional_set[set_name_to_work_on + "records"]["header"].erase("")
	if container_name_thing != "":
		data_to_work_on_additional_set[set_name_to_work_on + "records"]["header"].erase("")
	continue_recreation(data_to_work_on_additional_set, datapoint_name_thing, container_name_thing, set_name_to_work_on, current_container_count_check, record_type, amount_of_things, container_name)

func continue_recreation(data_to_work_on_additional_set, datapoint_name_thing, container_name_thing, set_name_to_work_on, current_container_count_check, record_type, amount_of_things, container_name):
	var just_thing : String = "thing_"
	var just_interaction : String = "interaction_"
	var just_path_now : String = container_name + "/" + just_thing
	var parts_to_change = BanksCombiner.data_names_3
	for part_now in parts_to_change:
		var part_name = set_name_to_work_on + part_now
		if data_to_work_on_additional_set.has(part_name):
			for stuff in data_to_work_on_additional_set[part_name]:
				if data_to_work_on_additional_set[part_name].has(stuff):
					var counting_int_0 : int = -1
					for recreation_0 in data_to_work_on_additional_set[part_name][stuff]:
						counting_int_0 +=1
						if stuff == "header":
							if recreation_0 is String:
								if recreation_0.begins_with(just_thing):
									var split_recreation_0 = recreation_0.split("_")
									var thing_name_split = split_recreation_0[0]
									var number_of_that_thing : int = int(split_recreation_0[1]) + amount_of_things
									var new_recreation_0 = thing_name_split + "_" + str(number_of_that_thing)
									data_to_work_on_additional_set[part_name][stuff][counting_int_0] = new_recreation_0
								elif recreation_0.begins_with(just_interaction):
									var split_recreation_0 = recreation_0.split("_")
									var interaction_name_split = split_recreation_0[0]
									var number_of_that_interaction : int = int(split_recreation_0[1]) + amount_of_things
									var new_recreation_0  = interaction_name_split + "_" + str(number_of_that_interaction)
									data_to_work_on_additional_set[part_name][stuff][counting_int_0] = new_recreation_0
						elif stuff == "content":
							var counting_int_1 : int = -1
							for stuff_to_find in recreation_0:
								counting_int_1 +=1
								if stuff_to_find is String:
									print(" recreator 2.0a we found string 0")
								else:
									var counting_int_2 : int = -1
									for stuff_to_find_0 in stuff_to_find:
										counting_int_2 +=1
										if stuff_to_find_0 is String:
											print(" recreator 2.0a we found string 1")
										else:
											var counting_int_3 : int = -1
											for stuff_to_find_1 in stuff_to_find_0:
												counting_int_3 +=1
												if stuff_to_find_1 is String:
													if stuff_to_find_1.begins_with(just_thing):
														if stuff_to_find_1 != datapoint_name_thing and stuff_to_find_1 != container_name_thing:
															var split_recreation_0 = stuff_to_find_1.split("_")
															var thing_name_split = split_recreation_0[0]
															var number_of_that_thing : int = int(split_recreation_0[1]) + amount_of_things
															var new_stuff_to_find_1 = thing_name_split + "_" + str(number_of_that_thing)
															data_to_work_on_additional_set[part_name][stuff][counting_int_0][counting_int_1][counting_int_2][counting_int_3] = new_stuff_to_find_1
													elif stuff_to_find_1.begins_with(just_interaction):
														var split_recreation_0 = stuff_to_find_1.split("_")
														var interaction_name_split = split_recreation_0[0]
														var number_of_that_interaction : int = int(split_recreation_0[1]) + amount_of_things
														var new_stuff_to_find_1 = interaction_name_split + "_" + str(number_of_that_interaction)
														data_to_work_on_additional_set[part_name][stuff][counting_int_0][counting_int_1][counting_int_2][counting_int_3] = new_stuff_to_find_1
													elif stuff_to_find_1.begins_with(just_path_now):
														var first_path_split = stuff_to_find_1.split("/")
														var second_split_of_thing = first_path_split[1].split("_")
														var first_merge = first_path_split[0] + "/" + second_split_of_thing[0] + "_"
														var number_of_that_thing : int = int(second_split_of_thing[1]) + amount_of_things
														var new_path_now = first_merge + str(number_of_that_thing)
														data_to_work_on_additional_set[part_name][stuff][counting_int_0][counting_int_1][counting_int_2][counting_int_3] = new_path_now
												else:
													print(" recreator 2.0a we could go deeper? ")
	var additional_set_name = record_type + str(current_container_count_check) + "_"
	active_r_s_mut.lock()
	active_record_sets[additional_set_name] = {}
	active_record_sets[additional_set_name][additional_set_name + "records"] = data_to_work_on_additional_set[set_name_to_work_on + "records"]
	active_record_sets[additional_set_name][additional_set_name + "scenes"] = data_to_work_on_additional_set[set_name_to_work_on + "scenes"]
	active_record_sets[additional_set_name][additional_set_name + "interactions"] = data_to_work_on_additional_set[set_name_to_work_on + "interactions"]
	active_record_sets[additional_set_name][additional_set_name + "instructions"] = data_to_work_on_additional_set[set_name_to_work_on + "instructions"]
	active_r_s_mut.unlock()

func create_record_from_script(record_type):
	var type_of_data : int
	var datapoint_node
	var records : Dictionary
	var current_data_pack_loaded
	var records_part : String
	var records_name : String
	records_part = ""
	match record_type:
		"base":
			current_data_pack_loaded = BanksCombiner.combination_0
			records_part = "base_"
		"menu":
			current_data_pack_loaded = BanksCombiner.combination_1
			records_part = "menu_"
		"settings":
			current_data_pack_loaded = BanksCombiner.combination_2
			records_part = "settings_"
		"keyboard":
			current_data_pack_loaded = BanksCombiner.combination_3
			records_part = "keyboard_"
		"keyboard_left":
			current_data_pack_loaded = BanksCombiner.combination_4
			records_part = "keyboard_left_"
		"keyboard_right":
			current_data_pack_loaded = BanksCombiner.combination_5
			records_part = "keyboard_right_"
		"things_creation":
			current_data_pack_loaded = BanksCombiner.combination_6
			records_part = "things_creation_"
		"singular_lines":
			current_data_pack_loaded = BanksCombiner.combination_7
			records_part = "singular_lines_"
		_:
			return {}
	for data_types in current_data_pack_loaded:
		type_of_data = data_types[0]
		match type_of_data:
			0:
				records = find_record_set(record_type)
				records_name = records_part + "records"
			1:
				records = find_instructions_set(record_type)
				records_name = records_part + "instructions"
			2: 
				records = find_scene_frames(record_type)
				records_name = records_part + "scenes"
			3:
				records = find_interactions_list(record_type)
				records_name = records_part + "interactions"
		load_record_set(records_part, records_name, type_of_data, records)
	process_creation_further(record_type, 1)

func find_record_set_new_file_finder(data):
	print("find it", data)

func process_creation_request(set_name: String) -> Dictionary:
	var result = {
		"status": CreationStatus.ERROR,
		"message": "",
		"timestamp": Time.get_ticks_msec()
	}
	if not is_creation_possible():
		result.status = CreationStatus.LOCKED
		result.message = "Creation system is not active"
		return result
	var creation_result = whip_out_set_by_its_name(set_name)
	match creation_result:
		CreationStatus.SUCCESS:
			result.status = CreationStatus.SUCCESS
			result.message = "Set created successfully"
		CreationStatus.ERROR:
			record_mistake({
				"type": "creation_error",
				"set_name": set_name,
				"error": "Creation failed"
			})
			result.message = "Failed to create set"
		_:
			result.message = "Unexpected creation status"
	return result

func prepare_akashic_records_init():
	first_start_check = "started"
	var main_sets_names = BanksCombiner.dataSetLimits
	var main_sets_names_just_names = BanksCombiner.data_sets_names_0
	var main_sets_names_with_underscore = BanksCombiner.data_sets_names
	array_of_startup_check.append(first_start_check)
	array_of_startup_check.append([["akashic_records"],["base"],["menu"]])
	var stuck_status = check_thread_status()
	before_time_blimp(0, 0)
	array_of_startup_check.append(main_sets_names)
	array_of_startup_check.append(main_sets_names_just_names)
	array_of_startup_check.append(main_sets_names_with_underscore)
	if stuck_status == "error":
		print(" timer check omething went wrong, use a timer")

func load_record_set(records_part: String, record_type: String, type_of_data : int, records : Dictionary) -> void:
	var max_nunmber_of_thingy = BanksCombiner.dataSetLimits[records_part]
	var current_number_of_that_set : int = 0
	if !active_record_sets.has(records_part):
		current_number_of_that_set = 1
	var list_of_reliquaries : Array = [] 
	var codices : Array = [] 
	var current_record_line : Array = []
	for current_record_to_process in records:
		var another_array_damn : Array = []
		var string_splitter
		for current_part in records[current_record_to_process]:
			string_splitter = current_part[0].split("|")
			var string_to_be_splitted
			var tomes_of_knowledge : Array = []
			for stringy_string in string_splitter:
				string_to_be_splitted = stringy_string.split(",")
				tomes_of_knowledge.append(string_to_be_splitted)
			current_record_line.append(string_splitter[0])
			another_array_damn.append(tomes_of_knowledge)
		codices.append(another_array_damn)
		list_of_reliquaries.append(current_record_line[0])
		current_record_line.clear()
	var string_header : String = "header"
	var string_content : String = "content"
	var records_processed : Dictionary = {}
	records_processed[string_header] =  list_of_reliquaries
	records_processed[string_content] = codices
	if active_record_sets.has(records_part):
		if active_record_sets[records_part].has(record_type):
			return
	if not active_record_sets.has(records_part):
		active_record_sets[records_part] = {
			"metadata": {
				"timestamp": Time.get_ticks_msec(),
				"container_count": current_number_of_that_set,
				"max_containers": max_nunmber_of_thingy
			}
		}
	if records.size() > 0:
		active_record_sets[records_part][record_type] = records_processed

#                                      ,,                                                          ,,          
# `7MMF'                             `7MM      `7MM"""Mq.                                        `7MM          
#   MM                                 MM        MM   `MM.                                         MM          
#   MM         ,pW"Wq.   ,6"Yb.   ,M""bMM        MM   ,M9  .gP"Ya   ,p6"bo   ,pW"Wq.`7Mb,od8  ,M""bMM  ,pP"Ybd 
#   MM        6W'   `Wb 8)   MM ,AP    MM        MMmmdM9  ,M'   Yb 6M'  OO  6W'   `Wb MM' "',AP    MM  8I   `" 
#   MM      , 8M     M8  ,pm9MM 8MI    MM        MM  YM.  8M"""""" 8M       8M     M8 MM    8MI    MM  `YMMMa. 
#   MM     ,M YA.   ,A9 8M   MM `Mb    MM        MM   `Mb.YM.    , YM.    , YA.   ,A9 MM    `Mb    MM  L.   I8 
# .JMMmmmmMMM  `Ybmd9'  `Moo9^Yo.`Wbmd"MML.    .JMML. .JMM.`Mbmmd'  YMbmd'   `Ybmd9'.JMML.   `Wbmd"MML.M9mmmP' 

# Load Records

func load_cached_data(data_set: String):
	var type_of_data : int
	var records_set_name = data_set
	active_r_s_mut.lock()
	var cached_data_new = active_record_sets[records_set_name].duplicate(true)
	active_r_s_mut.unlock()
	var thing_name
	var coords_to_place
	var direction_to_place
	var thing_type_file
	var shape_name
	var root_name
	var pathway_dna
	var group_number
	var counter_to_know : int = 0
	var first_line : Array = []
	var lines_parsed : Array = []
	for data_type in BanksCombiner.combination_new_gen_0:
		counter_to_know = 0
		type_of_data = int(data_type[0])
		var type_num = data_type[0]
		var data_name = records_set_name + BanksCombiner.data_names_0[type_num]
		var file_data = cached_data_new[data_name]["content"]
		var size_of_data = file_data.size()
		for record in file_data:
			counter_to_know +=1
			for lines in record:
				if lines == record[0]:
					first_line = record[0]
				else:
					lines_parsed.append(lines)
			match type_of_data:
				0:
					thing_name = first_line[0][0]
					coords_to_place = first_line[1][0]
					direction_to_place = first_line[2][0]
					thing_type_file = first_line[3][0]
					shape_name = first_line[4][0]
					root_name = first_line[5][0]
					pathway_dna = first_line[6][0]
					group_number = first_line[7][0]
				1:
					pass
				2:
					pass
				3:
					pass
			match type_of_data:
				0:
					analise_data(thing_name, thing_type_file, first_line, lines_parsed[0], group_number, shape_name, lines_parsed)
				1:
					print("instruction stuff:")
				2: 
					print(" scenes and frames analise : ")
				3: 
					print("so we will need to add them to datapoint")
					if counter_to_know - 666 == size_of_data:
						var container_node_path = first_line[1][0]
						var container_node = get_node(container_node_path)
						var datapoint_node = container_node.get_datapoint()
						var scene_number: int = 0
						datapoint_node.move_things_around(scene_number)
			first_line.clear()
			lines_parsed.clear()

#                                                                                                   ,,          
# `7MMM.     ,MMF'                              `7MM"""Mq.                                        `7MM          
#   MMMb    dPMM                                  MM   `MM.                                         MM          
#   M YM   ,M MM  ,pW"Wq.`7M'   `MF'.gP"Ya        MM   ,M9  .gP"Ya   ,p6"bo   ,pW"Wq.`7Mb,od8  ,M""bMM  ,pP"Ybd 
#   M  Mb  M' MM 6W'   `Wb VA   ,V ,M'   Yb       MMmmdM9  ,M'   Yb 6M'  OO  6W'   `Wb MM' "',AP    MM  8I   `" 
#   M  YM.P'  MM 8M     M8  VA ,V  8M""""""       MM  YM.  8M"""""" 8M       8M     M8 MM    8MI    MM  `YMMMa. 
#   M  `YM'   MM YA.   ,A9   VVV   YM.    ,       MM   `Mb.YM.    , YM.    , YA.   ,A9 MM    `Mb    MM  L.   I8 
# .JML. `'  .JMML.`Ybmd9'     W     `Mbmmd'     .JMML. .JMM.`Mbmmd'  YMbmd'   `Ybmd9'.JMML.   `Wbmd"MML.M9mmmP' 

# Move Records

func load_cached_record_to_active(records_set_name):

	active_r_s_mut.lock()
	cached_r_s_mutex.lock()
	active_record_sets[records_set_name] = cached_record_sets[records_set_name].duplicate(true)
	active_record_sets[records_set_name]["metadata"]["container_count"] +=1
	cached_record_sets.erase(records_set_name)
	active_r_s_mut.unlock()
	cached_r_s_mutex.unlock()

func deep_copy_dictionary(original: Dictionary) -> Dictionary:
	var json_string = JSON.stringify(original)
	var parsed = JSON.parse_string(json_string)
	return parsed

func clean_oldest_dataset() -> void:
	var oldest_time = Time.get_ticks_msec()
	var oldest_set = ""
	for timestamp_key in cache_timestamps:
		if cache_timestamps[timestamp_key] < oldest_time:
			oldest_time = cache_timestamps[timestamp_key]
			oldest_set = timestamp_key.split("_")[0]
	if oldest_set != "":
		cached_r_s_mutex.lock()
		cached_record_sets.erase(oldest_set + "_")
		cached_r_s_mutex.unlock()
		var to_remove = []
		for timestamp_key in cache_timestamps:
			if timestamp_key.begins_with(oldest_set):
				to_remove.append(timestamp_key)
		for key in to_remove:
			cache_timestamps.erase(key)

func process_to_unload_records(container_name_to_unload):
	var parts = container_name_to_unload.split("_")
	if parts.size() < 2:
		return
	var records_sets_name
	if parts.size() > 2:
		records_sets_name = parts[0] + "_" + parts[1]
	else:
		records_sets_name = parts[0]
	var counter_for_rec_ty : int = 0
	active_r_s_mut.lock()
	if active_record_sets[records_sets_name + "_" ].has("metadata"):
		active_record_sets[records_sets_name + "_" ]["metadata"]["container_count"] = 0
		active_r_s_mut.unlock()
		for records_types in BanksCombiner.combination_0:
			var record_to_unloadin = records_sets_name + "_" + BanksCombiner.data_names_0[counter_for_rec_ty]
			counter_for_rec_ty +=1
			unload_record_set(records_sets_name , record_to_unloadin)
		active_r_s_mut.lock()
		active_record_sets[records_sets_name + "_" ].erase("metadata")
		active_r_s_mut.unlock()
	else:
		active_r_s_mut.unlock()

func unload_record_set(records_sets_name : String, record_type: String) -> void:
	records_sets_name = records_sets_name + "_"
	active_r_s_mut.lock()
	if active_record_sets.has(records_sets_name):
		if active_record_sets[records_sets_name].has(record_type):
			var data = active_record_sets[records_sets_name][record_type]
			var meta_data = active_record_sets[records_sets_name]["metadata"]
			active_r_s_mut.unlock()
			cache_data(records_sets_name, record_type, data, meta_data)
			active_r_s_mut.lock()
			active_record_sets[records_sets_name].erase(record_type)
			active_r_s_mut.unlock()
		else:
			active_r_s_mut.unlock()
	else:
		active_r_s_mut.unlock()

func cache_data(records_sets_name: String, record_type: String, data, meta_data) -> void:
	var current_cache_size = get_cache_total_size()
	var new_data_size = get_dictionary_memory_size(data)
	var max_size_bytes = max_cache_size_mb * 1024 * 1024
	if current_cache_size + new_data_size > max_size_bytes:
		clean_oldest_dataset()
	current_cache_size = get_cache_total_size()
	cached_r_s_mutex.lock()
	if current_cache_size + new_data_size <= max_size_bytes:
		if !cached_record_sets.has(records_sets_name):
			active_r_s_mut.lock()
			cached_record_sets[records_sets_name] = { 
				"metadata": active_record_sets[records_sets_name]["metadata"].duplicate(true)
			}
			active_r_s_mut.unlock()
		cached_record_sets[records_sets_name][record_type] = data.duplicate(true)
		cached_record_sets[records_sets_name]["metadata"][str(record_type)] = {
			"size": new_data_size,
			"time_of_cache" : Time.get_ticks_msec()
		}
		cache_timestamps[records_sets_name + record_type] = Time.get_ticks_msec()
	else:
		print("Cache limit reached, cannot store new data")
	cached_r_s_mutex.unlock()

#               ,,                                                                                                      ,,          
#   .g8"""bgd `7MM                                                  `7MM"""Mq.                                        `7MM          
# .dP'     `M   MM                                                    MM   `MM.                                         MM          
# dM'       `   MMpMMMb.   ,6"Yb.  `7MMpMMMb.  .P"Ybmmm .gP"Ya        MM   ,M9  .gP"Ya   ,p6"bo   ,pW"Wq.`7Mb,od8  ,M""bMM  ,pP"Ybd 
# MM            MM    MM  8)   MM    MM    MM :MI  I8  ,M'   Yb       MMmmdM9  ,M'   Yb 6M'  OO  6W'   `Wb MM' "',AP    MM  8I   `" 
# MM.           MM    MM   ,pm9MM    MM    MM  WmmmP"  8M""""""       MM  YM.  8M"""""" 8M       8M     M8 MM    8MI    MM  `YMMMa. 
# `Mb.     ,'   MM    MM  8M   MM    MM    MM 8M       YM.    ,       MM   `Mb.YM.    , YM.    , YA.   ,A9 MM    `Mb    MM  L.   I8 
#   `"bmmmd'  .JMML  JMML.`Moo9^Yo..JMML  JMML.YMMMMMb  `Mbmmd'     .JMML. .JMM.`Mbmmd'  YMbmd'   `Ybmd9'.JMML.   `Wbmd"MML.M9mmmP' 
#                                             6'     dP                                                                             
#                                             Ybmmmd'                                                                               

# Change Records

func add_container_count(records_set_name):
	active_r_s_mut.lock()
	if active_record_sets.has(records_set_name):
		if active_record_sets[records_set_name].has("metadata"):
			if active_record_sets[records_set_name]["metadata"].has("container_count"):
				active_record_sets[records_set_name]["metadata"]["container_count"] +=1
			else:
				print(" metadata has no container count ")
		else:
			print(" set has no metadata ")
	else:
		print(" we dont have that set ")
	active_r_s_mut.unlock()

func recreator(number_to_add, data_to_process, data_set_name, new_name_for_set):
	var initial_number_to_add : int = int(number_to_add)
	print(" recreator whats wrong")
	print(" new_name_for_set : " , new_name_for_set)
	var processed_data : Dictionary
	var data_to_work_on = data_to_process.duplicate(true)
	var container_path = data_set_name + "_container/thing_"
	var patterns = ["thing_" , container_path ]
	var number_we_wanna_add : int
	var container_name_to_free
	var data_type_name_combined_first = data_set_name + "_" + BanksCombiner.data_names_0[0]
	var tasks_to_be_done : int = 0
	var datapoint_name
	var datapoint_container_name
	for container_to_find in data_to_work_on[data_type_name_combined_first]["content"]:
		#for
		if container_to_find[0][3][0] == "container":
			container_name_to_free = container_to_find[0][0][0]
			container_to_find.clear()
			#break
			break
	data_to_work_on[data_type_name_combined_first]["content"].erase([])
	#for
	for data_types in BanksCombiner.data_names_0:
		var data_type_name_combined = data_set_name + "_" + data_types
		print(data_set_name + "_" + data_types)
		for data_to_be_parsed_1 in data_to_work_on[data_type_name_combined]: 
			if data_to_be_parsed_1 == "header":
				if BanksCombiner.data_names_0[0] == data_types:
					number_we_wanna_add = data_to_work_on[data_type_name_combined][data_to_be_parsed_1].size()
					var counter_for_header_strings : int = 0
					for container_name_to_find in data_to_work_on[data_type_name_combined][data_to_be_parsed_1]:
						if container_name_to_find == container_name_to_free:
							container_name_to_find = ""
							data_to_work_on[data_type_name_combined][data_to_be_parsed_1][counter_for_header_strings] = ""
							data_to_work_on[data_type_name_combined][data_to_be_parsed_1].erase("")
							counter_for_header_strings +=1
							break
			var counter_new_0 : int = 0
			for data_to_be_parsed_2 in data_to_work_on[data_type_name_combined][data_to_be_parsed_1]:
				if data_to_be_parsed_2 is String:
					for pattern in patterns:
						if data_to_be_parsed_2.begins_with(pattern):
							var string_to_change = data_to_be_parsed_2.split("_")
							var size_of_array = string_to_change.size() -1
							string_to_change[size_of_array] = str(int(string_to_change[size_of_array]) + number_we_wanna_add)
							string_to_change = "_".join(string_to_change)
							data_to_work_on[data_type_name_combined][data_to_be_parsed_1][counter_new_0] = string_to_change
				if data_to_be_parsed_2 is Array:
					print(" recreator data_types : " , data_types)
					if data_types == "instructions":
						print(" recreator_check : 0 : ", data_to_be_parsed_2[0][1][0])
						print(" recreator_check : 0 : ", data_to_be_parsed_2[2][0][0])
						if data_to_be_parsed_2[0][1][0] == "set_the_scene":
							print(" recreator_check : 0 :  we found that set the scene " , number_to_add , " for   " , new_name_for_set)
							data_to_be_parsed_2[2][0][0][0] = str(number_to_add)
							#break
					if initial_number_to_add == 1:
						if data_types == "scenes":
							print(" recreator_check : 10 : ", data_to_be_parsed_2[0][0], " and number_to_add : " , number_to_add)
							print(" recreator_check : 11 : ", data_to_be_parsed_2)
							var scene_number = data_to_be_parsed_2[0][0][0].substr(6, data_to_be_parsed_2[0][0][0].length()) #scene)
							print(" recreator_check : 12 : ", scene_number)
							number_to_add = scene_number
							print(" recreator_check : 14 number_to_add " , number_to_add)
						if data_types == "interactions":
							number_to_add = initial_number_to_add
							print(" recreator_check : 15 number_to_add " , number_to_add)
					else:
						break
					print(" recreator data_types : continuation : " , data_types)
					if data_to_be_parsed_2.size() > 1:
						var counter_new_1 : int = 0
						var counter_helper : int = 0
						for data_to_be_parsed_3 in data_to_be_parsed_2:
							if data_to_be_parsed_3 is String:
								for pattern in patterns:
									if data_to_be_parsed_3.begins_with(pattern):
										var string_to_change = data_to_be_parsed_3.split("_")
										var size_of_array = string_to_change.size() -1
										string_to_change[size_of_array] = str(int(string_to_change[size_of_array]) + number_we_wanna_add)
										string_to_change = "_".join(string_to_change)
										data_to_be_parsed_3 = string_to_change
										counter_helper +=1
							if data_to_be_parsed_3 is Array:
								if data_to_be_parsed_3.size() > 1:
									var counter_new_2 : int = 0
									for data_to_be_parsed_4 in data_to_be_parsed_3:
										if data_to_be_parsed_4[0] is String:
											for pattern in patterns:
												if data_to_be_parsed_4[0].begins_with(pattern):
													var string_to_change = data_to_be_parsed_4[0].split("_")
													var size_of_array = string_to_change.size() -1
													string_to_change[size_of_array] = str(int(string_to_change[size_of_array]) + number_we_wanna_add)
													string_to_change = "_".join(string_to_change)
													data_to_be_parsed_4[0] = string_to_change
										counter_new_2 +=1
							counter_new_1 +=1
				counter_new_0 +=1
				
	#for
	for container_to_find in data_to_work_on[data_type_name_combined_first]["content"]:
		if container_to_find[0][3][0] == "datapoint":
			datapoint_name = container_to_find[0][0][0]
			datapoint_container_name = container_to_find[0][5][0]
			#break
			break
	#for
	for data_types in BanksCombiner.data_names_0:
		var data_type_name_combined = data_set_name + "_" + data_types
		var data_type_name_combined_new = new_name_for_set + data_types
		print(data_set_name + "_" + data_types)
		#for
		for data_to_be_parsed_1 in data_to_work_on[data_type_name_combined]: 
			processed_data[data_type_name_combined_new] = data_to_work_on[data_type_name_combined].duplicate(true)
	processed_data["metadata"] = {
				"timestamp": Time.get_ticks_msec(),
				"datapoint_name": datapoint_name,
				"datapoint_container_name": datapoint_container_name
			} 
	print(" recreator : ", processed_data)
	##return
	return processed_data

